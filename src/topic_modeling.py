# Importing libraries
import os
import openai
from openai import AzureOpenAI
import json
import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from bertopic import BERTopic
from bertopic.representation import KeyBERTInspired, PartOfSpeech, MaximalMarginalRelevance
from bertopic.backend import OpenAIBackend
from umap import UMAP
from hdbscan import HDBSCAN
from typing import List, Dict


class BERTTopicModeler:
    def __init__(self):
        """
        Initializes the model with the necessary API keys and configures external services.

        """
        self.client = AzureOpenAI(
            api_version="2024-02-01",
            azure_endpoint="https://genai-nexus.api.corpinter.net/apikey/",
            api_key=os.getenv("OPENAI_ADA")
        )
        self.openai_embedder = OpenAIBackend(self.client, "text-embedding-ada-002")
        self.model = None
        self.client_text = AzureOpenAI(
            api_version=os.getenv("OPENAI_API_VERSION"),
            azure_endpoint=os.getenv("OPENAI_ENDPOINT"),
            api_key=os.getenv("NEXT_API_KEY")
        )

    def clustering(self, documents: List[str], seed: int) -> Dict:
        """
        Performs clustering of documents using BERTopic.

        Args:
            documents (List[str]): List of text documents to cluster.
            granularity (bool): If True, uses fine-grained configuration.
            seed (int): Seed for reproducibility.
        
        Returns:
            Dict: Dictionary with generated topic information.
        """
        # Calculating number of documents
        num_documents = len(documents)
        
        # Adjust UMAP parameters
        n_neighbors = max(2, min(15, int(0.05 * num_documents)))
        n_components = 5 if num_documents < 500 else 10
        umap_model = UMAP(
            n_neighbors=n_neighbors,
            n_components=n_components,
            min_dist=0.0,
            metric='cosine',
            random_state=seed
        )
        
        # Adjust HDBSCAN parameters
        min_cluster_size = max(2, int(0.01 * num_documents))
        min_samples = max(1, int(0.002 * num_documents))
        hdbscan_model = HDBSCAN(
            min_cluster_size=min_cluster_size,
            min_samples=min_samples,
            metric="euclidean",
            cluster_selection_method='eom',
            prediction_data=True
        )
        
        # Adjust ngram_range and min_df
        if num_documents < 100:
            ngram_range = (1, 1)
            min_df = 1
        elif num_documents < 1000:
            ngram_range = (1, 2)
            min_df = 2
        else:
            ngram_range = (1, 3)
            min_df = 5
        
        # Define representation models for topics
        main_representation_model = KeyBERTInspired(
            top_n_words=10,
            nr_repr_docs=5,
            nr_samples=50,
            random_state=seed
        )
        aspect_representation_model1 = PartOfSpeech(
            model="en_core_web_sm",
            top_n_words=10
        )
        aspect_representation_model2 = [
            KeyBERTInspired(top_n_words=20),
            MaximalMarginalRelevance(diversity=0.3)
        ]

        representation_model = {
            "Main": main_representation_model,
            "Aspect1": aspect_representation_model1,
            "Aspect2": aspect_representation_model2
        }

        # Configure the vectorizer and BERTopic model
        vectorizer_model = CountVectorizer(
            min_df=min_df,
            stop_words='english',
            ngram_range=ngram_range
        )
        self.model = BERTopic(
            umap_model=umap_model,
            hdbscan_model=hdbscan_model,
            top_n_words=15,
            calculate_probabilities=True,
            vectorizer_model=vectorizer_model,
            representation_model=representation_model,
            embedding_model=self.openai_embedder,
            verbose=True
        )

        print("Fitting the BERTopic model...")
        topics, probs = self.model.fit_transform(documents)

        # Extract topic information
        topic_info = self.model.get_topic_info()
        topic_descriptions = self._define_topics(topic_info)
        
        return {
            "model": self.model,
            "topics": topics,
            "topic_info": topic_info,
            "topic_descriptions": topic_descriptions
        }

    def _define_topics(self, topic_info: pd.DataFrame) -> Dict:
        """
        Defines the topics using the clustering results and GPT-3 from OpenAI.

        Args:
            topic_info (pd.DataFrame): Topic information generated by BERTopic.
        
        Returns:
            Dict: Topic descriptions.
        """
        topic_dict = {}
        for index, row in topic_info.iterrows():
            all_keywords = ' '.join(row["Representation"]) + ' ' + ' '.join(row["Aspect1"]) + ' ' + ' '.join(row["Aspect2"])
            representative_documents = ", ".join(row["Representative_Docs"])
            # Call function to define the topic using GPT-3 or GPT-4
            repr_topic = self._call_openai_to_define_topic(all_keywords, representative_documents)
            topic_dict[index] = {
                "representative_topic": repr_topic,
                "count": row["Count"],
                "documents": row["Representative_Docs"]
            }
        return topic_dict

    def _call_openai_to_define_topic(self, keywords: str, representative_documents: str) -> str:
        """
        Calls OpenAI's GPT-3/4 model to define a topic based on keywords and representative documents.

        Args:
            keywords (str): Keywords defining the topic.
            representative_documents (str): Representative documents for the topic.
        
        Returns:
            str: Topic definition generated by GPT model.
        """
        prompt = f"""I have a topic that contains the following documents: {representative_documents}
            The topic is described by the following keywords=:{keywords}
            Based on the given information, generate a descriptive label for the topic, that captures the specific issues, without it being too generalized.
            Generate the label in the following format, and make sure that the topic label is not longer than 6 words."""
        
        response = self.client_text.chat.completions.create(
            model="gpt4-turbo",  # e.g. gpt-35-instant
            messages=[
                {
                    "role": "user",
                    "content": prompt,
                },
            ],
        )

        return response.choices[0].message.content
    
    def _postprocess_with_topics(self, data: List[Dict], topics: List[int]) -> List[Dict]:
        """
        Adds the corresponding topic number to each element in the metadata of the input data.

        Args:
            data (List[Dict]): The original data, a list of dictionaries with "sentence" and "metadata".
            topics (List[int]): The list of topics corresponding to each item in the data.

        Returns:
            List[Dict]: The updated data with the "topic" added to the metadata of each item.
        """
        if len(data) != len(topics):
            raise ValueError("The length of data and topics must match.")
    
        # Loop over each item in data and add the corresponding topic to its metadata
        for i, item in enumerate(data):
            item['metadata']['topic'] = topics[i]
        
        return data

    def unsupervised_bertopic(self, data: List[Dict], seed: int = 42):
        """
        Performs unsupervised topic modeling using BERTopic.

        Args:
            data (List[Dict]): List of dictionaries containing the sentences to process.
            result_path (str): Path to store results.
            seed (int, optional): Seed for reproducibility. Defaults to 42.
        """
        # Extract only the sentences from the data set
        sentences = [item["sentence"] for item in data]

        print("Starting clustering...")
        clustering_result = self.clustering(sentences, seed=seed)

        # Postprocessing results
        print("Postprocessing data...")
        postprocessed_data = self._postprocess_with_topics(data=data,topics=clustering_result["topics"])

        return postprocessed_data, clustering_result["topic_descriptions"]

if __name__ == "__main__":

    # Example data
    from data_preprocessing import Preprocessor
    paper_folder = r"/Users/sduran/Desktop/carpeta sin tiÃÅtulo"

    preprocessor = Preprocessor(paper_folder)
    preprocessor.enumerate_files()
    preprocessor.process_pdfs()
    data = preprocessor.data

    
    # Initialize the topic modeler
    topic_modeler = BERTTopicModeler()
    
    # Perform clustering and save the results
    topic_modeler.unsupervised_bertopic(data)