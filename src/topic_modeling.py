# Importing libraries
import os
import openai
from openai import AzureOpenAI
import json
import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from bertopic import BERTopic
from bertopic.representation import KeyBERTInspired, PartOfSpeech, MaximalMarginalRelevance
from bertopic.backend import OpenAIBackend
from umap import UMAP
from hdbscan import HDBSCAN
from typing import List, Dict


class BERTTopicModeler:
    def __init__(self):
        """
        Initializes the model with the necessary API keys and configures external services.

        """
        # self.client = openai.OpenAI(api_key=os.getenv("OPENAI_ADA"))
        self.client = AzureOpenAI(
            api_version="2024-02-01",
            azure_endpoint="https://genai-nexus.api.corpinter.net/apikey/",
            api_key=os.getenv("OPENAI_ADA")
        )
        self.openai_embedder = OpenAIBackend(self.client, "text-embedding-ada-002")
        self.model = None

    def clustering(self, documents: List[str], seed: int) -> Dict:
        """
        Performs clustering of documents using BERTopic.

        Args:
            documents (List[str]): List of text documents to cluster.
            granularity (bool): If True, uses fine-grained configuration.
            seed (int): Seed for reproducibility.
        
        Returns:
            Dict: Dictionary with generated topic information.
        """
        umap_params = [{'n_neighbors': 50, 'n_components': 10, 'min_dist': 0, 'metric': 'cosine'}]
        hdbscan_params = [{'min_cluster_size': 2, 'min_samples': 3}]
        ngram_range = (1, 3)
        
        # Create UMAP and HDBSCAN models
        umap_model = UMAP(**umap_params[0])
        hdbscan_model = HDBSCAN(**hdbscan_params[0], metric="euclidean", cluster_selection_method='eom', prediction_data=True)
        
        # Define representation models for topics
        main_representation_model = KeyBERTInspired(top_n_words=10, nr_repr_docs=5, nr_samples=50, random_state=seed)
        aspect_representation_model1 = PartOfSpeech(model="en_core_web_sm", top_n_words=10)
        aspect_representation_model2 = [KeyBERTInspired(top_n_words=20), MaximalMarginalRelevance(diversity=0.3)]
        
        representation_model = {
            "Main": main_representation_model,
            "Aspect1": aspect_representation_model1,
            "Aspect2": aspect_representation_model2
        }

        # Configure the vectorizer and BERTopic model
        vectorizer_model = CountVectorizer(min_df=3, stop_words='english', ngram_range=ngram_range)
        self.model = BERTopic(
            umap_model=umap_model,
            hdbscan_model=hdbscan_model,
            top_n_words=15,
            calculate_probabilities=True,
            vectorizer_model=vectorizer_model,
            representation_model=representation_model,
            embedding_model=self.openai_embedder,
            verbose=True
        )

        print("Fitting the BERTopic model...")
        topics, probs = self.model.fit_transform(documents)
        new_topics = self.model.reduce_outliers(documents, topics, probs)
        self.model.update_topics(documents, new_topics)

        # Extract topic information
        topic_info = self.model.get_topic_info()
        topic_descriptions = self._define_topics(topic_info)
        
        return {
            "model": self.model,
            "topics": new_topics,
            "topic_info": topic_info,
            "topic_descriptions": topic_descriptions
        }

    def _define_topics(self, topic_info: pd.DataFrame) -> Dict:
        """
        Defines the topics using the clustering results and GPT-3 from OpenAI.

        Args:
            topic_info (pd.DataFrame): Topic information generated by BERTopic.
        
        Returns:
            Dict: Topic descriptions.
        """
        topic_dict = {}
        for index, row in topic_info.iterrows():
            all_keywords = ' '.join(row["Representation"]) + ' ' + ' '.join(row["Aspect1"]) + ' ' + ' '.join(row["Aspect2"])
            representative_documents = ", ".join(row["Representative_Docs"])
            # Call function to define the topic using GPT-3 or GPT-4
            repr_topic = self._call_openai_to_define_topic(all_keywords, representative_documents)
            topic_dict[index] = {
                "representative_topic": repr_topic,
                "count": row["Count"],
                "documents": row["Representative_Docs"]
            }
        return topic_dict

    def _call_openai_to_define_topic(self, keywords: str, representative_documents: str) -> str:
        """
        Calls OpenAI's GPT-3/4 model to define a topic based on keywords and representative documents.

        Args:
            keywords (str): Keywords defining the topic.
            representative_documents (str): Representative documents for the topic.
        
        Returns:
            str: Topic definition generated by GPT model.
        """
        prompt = f"""I have a topic that contains the following documents: {representative_documents}
            The topic is described by the following keywords=:{keywords}
            Based on the given information, generate a descriptive label for the topic, that captures the specific issues, without it being too generalized.
            Generate the label in the following format, and make sure that the topic label is not longer than 6 words."""
        
        response = self.client.Completion.create(
            engine="gpt4-turbo",
            prompt=prompt,
            max_tokens=150,
            temperature=0
        )
        return response.choices[0].text.strip()

    def save_topics_to_json(self, topic_descriptions: Dict, result_path: str):
        """
        Saves the topic descriptions to a JSON file.

        Args:
            topic_descriptions (Dict): Topic descriptions generated by BERTopic.
            result_path (str): Path where the JSON file will be stored.
        """
        with open(os.path.join(result_path, "topic_data.json"), 'w') as json_file:
            json.dump(topic_descriptions, json_file, indent=4)

    def unsupervised_bertopic(self, data: List[Dict], result_path: str, seed: int = 42):
        """
        Performs unsupervised topic modeling using BERTopic.

        Args:
            data (List[Dict]): List of dictionaries containing the sentences to process.
            result_path (str): Path to store results.
            seed (int, optional): Seed for reproducibility. Defaults to 42.
        """
        # Extract only the sentences from the data set
        sentences = [item["sentence"] for item in data]

        print("Starting clustering...")
        clustering_result = self.clustering(sentences, seed=seed)
        
        # Save the topic descriptions in a JSON file
        self.save_topics_to_json(clustering_result["topic_descriptions"], result_path)

if __name__ == "__main__":

    # Example data
    data = [
        {"sentence": "This is the first sentence.", "metadata": {}},
        {"sentence": "Another sentence to cluster.", "metadata": {}},
        {"sentence": "Yet another one for topic modeling.", "metadata": {}}
    ]
    
    result_path = "./results"
    
    # Initialize the topic modeler
    topic_modeler = BERTTopicModeler()
    
    # Perform clustering and save the results
    topic_modeler.unsupervised_bertopic(data, result_path)